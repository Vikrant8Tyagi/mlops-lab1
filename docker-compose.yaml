services:
  # --- Lab 10: Model Serving (FastAPI) ---
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    container_name: fast_api
    ports:
      - "8000:8000"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow_server:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_DEFAULT_REGION=us-east-1
    depends_on:
      - mlflow

  # --- Lab 10: Monitoring (Prometheus) ---
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
    depends_on:
      - api

  # --- Lab 10: Visualization (Grafana) ---
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    depends_on:
      - prometheus

  # --- Lab 2: CI Server ---
  # jenkins:
  #   image: jenkins/jenkins:lts
  #   user: root
  #   ports:
  #     - "8080:8080"
  #     - "50000:50000"
  #   environment:
  #     - JAVA_OPTS=-Dhudson.plugins.git.GitSCM.ALLOW_LOCAL_CHECKOUT=true
  #   volumes:
  #     - ./jenkins_home:/var/jenkins_home
  #     - ./:/app
  #   working_dir: /app

  # --- Lab 3: Artifact Store (S3) ---
  minio:
    image: minio/minio
    container_name: mlflow_s3
    ports:
      - "9000:9000"
      - "9001:9001"
    command: server /data --console-address ":9001"
    environment:
      - MINIO_ROOT_USER=minioadmin
      - MINIO_ROOT_PASSWORD=minioadmin
    volumes:
      - minio_data:/data

  # --- Lab 3: Experiment Tracking ---
  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow_server
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - MLFLOW_S3_IGNORE_TLS=true
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_DEFAULT_REGION=us-east-1
      - AWS_REGION=us-east-1
    # Fix: Install boto3 at runtime for S3 connectivity
    command: >
      sh -c "pip install boto3 && mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root s3://mlflow/ --host 0.0.0.0 --allowed-hosts '*' --cors-allowed-origins '*'"
    depends_on:
      - minio

  # --- Lab 6: Message Broker ---
  # zookeeper:
  #   image: confluentinc/cp-zookeeper:7.4.0
  #   container_name: zookeeper
  #   environment:
  #     ZOOKEEPER_CLIENT_PORT: 2181
  #     ZOOKEEPER_TICK_TIME: 2000
  #   ports:
  #     - "2181:2181"

  # kafka:
  #   image: confluentinc/cp-kafka:7.4.0
  #   container_name: kafka
  #   depends_on:
  #     - zookeeper
  #   ports:
  #     - "9092:9092"
  #   environment:
  #     KAFKA_BROKER_ID: 1
  #     KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
  #     KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
  #     KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
  #     KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  #     KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # --- Lab 7: Airflow Backend (Postgres) ---
  airflow-db:
    image: postgres:13
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    ports:
      - "5432:5432"

  # --- Lab 7: Airflow Init (Runs Once) ---
  airflow-init:
    build: .
    container_name: airflow_init
    depends_on:
      - airflow-db
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
    command: >
      bash -c " airflow db init && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true "

  # --- Lab 7: Airflow Scheduler & Webserver ---
  airflow:
    build: . # <--- USES LOCAL DOCKERFILE
    container_name: airflow
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      mlflow:
        condition: service_started
    ports:
      - "8081:8080" # Host 8081 -> Container 8080 (Avoids Jenkins conflict)
    environment:
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-db/airflow
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      # MLOps Connections
      - MLFLOW_TRACKING_URI=http://mlflow_server:5000
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=minioadmin
      - AWS_SECRET_ACCESS_KEY=minioadmin
      - AWS_DEFAULT_REGION=us-east-1
    volumes:
      - ./dags:/opt/airflow/dags
      - ./:/opt/airflow/project # Mount code for execution
      - ./data:/opt/airflow/project/data # Mount data
    command: >
      bash -c "airflow webserver & airflow scheduler"

volumes:
  minio_data:
